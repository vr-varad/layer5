{"componentChunkName":"component---src-templates-blog-single-js","path":"/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally.html","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Docker Compose: Orchestrating Multi-Service AI Applications Locally\",\n  \"subtitle\": \"Docker Model Runner - A Technical Primer for Engineers\",\n  \"date\": \"2025-04-24 10:30:05 -0530\",\n  \"author\": \"Lee Calcote\",\n  \"thumbnail\": \"./hero-image.png\",\n  \"darkthumbnail\": \"./hero-image.png\",\n  \"category\": \"Docker\",\n  \"tags\": [\"docker\", \"ai\"],\n  \"type\": \"Blog\",\n  \"resource\": true,\n  \"published\": true\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(BlogWrapper, {\n    mdxType: \"BlogWrapper\"\n  }, mdx(\"p\", null, \"So far in our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/blog/category/docker\"\n  }, \"series on Docker Model Runner\"), \", we've dissected its OCI-based model management, its performance-optimized execution architecture, and its OpenAI-compatible API. Now, we explore a feature that truly elevates its utility for engineers building complex systems: \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep integration with Docker Compose via a novel provider service type.\"), \"  \"), mdx(\"p\", null, \"For engineers, Docker Compose is the go-to tool for defining and running multi-container Docker applications. The introduction of the provider service type specifically for Model Runner bridges the gap between local AI model execution and the broader application stack, allowing you to define and manage AI models as integral components of your local development environment declaratively.\"), mdx(\"h2\", null, mdx(\"strong\", {\n    parentName: \"h2\"\n  }, \"Beyond CLI: Models as First-Class Services in Compose\")), mdx(\"p\", null, \"While docker model run is handy for quick tests, real-world applications often involve multiple interacting services\\u2014a web frontend, a backend API, a database, and now, an AI model. Docker Model Runner's Compose integration allows you to define the AI model itself as a service within your \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"docker-compose.yml\"), \" file.  \"), mdx(\"p\", null, \"The key innovation here is the provider attribute within a service definition. Here's a conceptual example based on Docker's documentation:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"services:  \\n  model\\\\_provider\\\\_service: \\\\# You can name this service as you like  \\n    provider:  \\n      type: model        \\\\# Specifies this is a model provider  \\n      image: ai/llama3.2:1B-Q8\\\\_0 \\\\# The OCI image for the model  \\n    \\\\# No 'build' or 'image' directives here in the traditional sense for the provider\\n\\n  my\\\\_app\\\\_service:  \\n    build: ./app  \\n    ports:  \\n      \\\\- \\\"8080:80\\\"  \\n    depends\\\\_on:  \\n      \\\\- model\\\\_provider\\\\_service \\\\# Ensures model is ready before the app starts  \\n    environment:  \\n      \\\\# Environment variables will be injected here (see below)  \\n      MODEL\\\\_NAME: ${MODEL\\\\_PROVIDER\\\\_SERVICE\\\\_MODEL}  \\n      MODEL\\\\_URL: ${MODEL\\\\_PROVIDER\\\\_SERVICE\\\\_URL}\\n\")), mdx(\"p\", null, \"In this setup:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"model\", \"_\", \"provider\", \"_\", \"service doesn't run a traditional container in the same way my\", \"_\", \"app\", \"_\", \"service does. Instead, it instructs Docker Compose to leverage Docker Model Runner.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Docker Model Runner, when processing this provider service, will ensure the specified image (the AI model) is pulled and made available via its host-native inference engine.\")), mdx(\"h2\", null, mdx(\"strong\", {\n    parentName: \"h2\"\n  }, \"Automatic Model Provisioning and Service Discovery\")), mdx(\"p\", null, \"This Compose integration brings significant benefits for engineers:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Declarative Model Dependencies:\"), \"  \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You declare your AI model dependency directly in your docker-compose.yml. Docker Model Runner handles the provisioning (pulling and preparing the model if needed) when you run docker compose up.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is a stark improvement over manual docker model run commands or custom scripts to manage model lifecycle alongside your application stack.  \"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Automated Service Discovery via Environment Variables:\"), \"  \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is a crucial feature for seamless integration. When my\", \"_\", \"app\", \"_\", \"service starts (after model\", \"_\", \"provider\", \"_\", \"service is ready), Docker Compose automatically injects environment variables into my\", \"_\", \"app\", \"_\", \"service.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"These variables typically follow the pattern: PROVIDER\", \"_\", \"SERVICE\", \"_\", \"NAME\", \"_\", \"MODEL and PROVIDER\", \"_\", \"SERVICE\", \"_\", \"NAME\", \"_\", \"URL.  \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MODEL\", \"_\", \"PROVIDER\", \"_\", \"SERVICE\", \"_\", \"MODEL: Contains the name/tag of the model being served (e.g., ai/llama3.2:1B-Q8\", \"_\", \"0).  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MODEL\", \"_\", \"PROVIDER\", \"_\", \"SERVICE\", \"_\", \"URL: Provides the URL your application should use to access the Model Runner's API endpoint for this model. This would often point to the internal DNS \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://model-runner.docker.internal\"\n  }, \"http://model-runner.docker.internal\"), \" or a host-accessible TCP port if configured.  \"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Your application code can then dynamically use these environment variables to configure its AI client, making the connection to the local model effortless and portable.  \"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Simplified depends\", \"_\", \"on for Startup Order:\"), \"  \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Using depends\", \"_\", \"on ensures that your application services only start after Model Runner has signaled that the model provider is ready. This prevents your application from trying to connect to a model that isn't yet available.\")))), mdx(\"h2\", null, mdx(\"strong\", {\n    parentName: \"h2\"\n  }, \"Engineering Benefits for Complex AI Applications\")), mdx(\"p\", null, \"This declarative, integrated approach offers tangible advantages:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Reproducible AI Development Environments:\"), \" Your entire local stack, including the specific AI model version, is defined in code (docker-compose.yml), making it easy to share, version control, and ensure consistency across development team members.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Simplified Onboarding:\"), \" New developers can get a complex AI-powered application stack running locally with a single docker compose up command.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Streamlined Local Testing of AI Features:\"), \" Test end-to-end flows involving your application logic and AI model interactions in a fully integrated local environment that mirrors how services would interact.  \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Foundation for Local MLOps Loops:\"), \" While focused on local development, this pattern lays a conceptual foundation for how AI models can be treated as manageable dependencies within larger application architectures, aligning with MLOps principles.\")), mdx(\"p\", null, \"By treating AI models as discoverable services managed by Compose, Docker Model Runner significantly lowers the barrier to building and iterating on sophisticated multi-service applications that leverage local AI capabilities. This moves beyond simply running a model in isolation to truly integrating AI into your development workflow.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"Next up, we'll explore how Docker Model Runner specifically caters to Java developers through its integration with frameworks like Spring AI, further simplifying the adoption of local AI.  \"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.\"))));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Docker Compose: Orchestrating Multi-Service AI Applications Locally","subtitle":"Docker Model Runner - A Technical Primer for Engineers","description":null,"date":"April 24th, 2025","author":"Lee Calcote","category":"Docker","tags":["docker","ai"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42jWL209SAQDGz1/Rk+VDq1aGpQk6BW2o6NDkKiTZFJAIsABJwbiIoAgcOQeO4EFuAuq84FDIynJuqZslm6WUa5lTmm3OFfrQW08d59p++/b9tu8DQqfR4GnkjGwkeBLBVLPUr5jXeQ7DhvURZHdq/M+c7zTsz4b82TA2wAicnJcwgBygrn3UkwkMH4Y8P4LQN/Qmt6HerJfPT+D4WmlsciAWcyzNIpno8E+/+8A/lAm4/wMEf6WQ/Tn7F79hGbZto/aU9yHseBx5LZ5ISkYjzoCprktlDIxb4vH2mX5wxzuwhdrSXus2ak2jQN9WDN7fmPx79DQwRWnp4um8DwSWwnrJxQrm3YbniMJr29s2H3+FkottPrf544huza1f8ehW3fo1D9C/s2LYmFcv+jhGa04+s5CqqpX7KyXQ5Sp2GVXmV010WMM8KGGZWVVPL90HbfK4s+PVkCKJKBeGAGHE2Zve7E2nJCNBls5xncS5w1ZWCuw1T1CqzKexzT2yTFHEo0zNLA9+IzCPkVsNLX6obcwliLoAjhPiumDF2+Wed+sMZW85Q1zKU4gMwxRe97Xy1oI6OaszZEtsBJY/d7gTL9/vhl9siqYTXATiOGGAZoZpJked3krrg8kKfXGzNK+WV0AT3Wvvq1eZyO3aS0VsYTQZzByv7f7+9D2b2jvSji9Qe0C62QlUqxwYlGcwRQXVqhFKp7NCai5gS3JLGxrVlq5gvKkbMgYWzEsfdOGEVAsTyNx8IrtKCWIXgMgHSQKQyLeTBHYsia02knCwQgSX8EyExs5KvgZfzRNKrQy28sYtyoXckpy8GhxVhr2IAhDA0+14BgZ4XghM8EzpNgJrEM9y4GqMV4iynNtNuUXNV8vEuGp1Ed1azIbO9gzwH9t6T8ElxPFqAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg","srcSet":"/static/97d66df31989169322ece96b70bfd7a8/9976b/hero-image.jpg 125w,\n/static/97d66df31989169322ece96b70bfd7a8/e19ca/hero-image.jpg 250w,\n/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg 500w,\n/static/97d66df31989169322ece96b70bfd7a8/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/97d66df31989169322ece96b70bfd7a8/842b0/hero-image.webp 125w,\n/static/97d66df31989169322ece96b70bfd7a8/b597c/hero-image.webp 250w,\n/static/97d66df31989169322ece96b70bfd7a8/49c7e/hero-image.webp 500w,\n/static/97d66df31989169322ece96b70bfd7a8/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}},"extension":"png","publicURL":"/static/97d66df31989169322ece96b70bfd7a8/hero-image.png"},"darkthumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42jWL209SAQDGz1/Rk+VDq1aGpQk6BW2o6NDkKiTZFJAIsABJwbiIoAgcOQeO4EFuAuq84FDIynJuqZslm6WUa5lTmm3OFfrQW08d59p++/b9tu8DQqfR4GnkjGwkeBLBVLPUr5jXeQ7DhvURZHdq/M+c7zTsz4b82TA2wAicnJcwgBygrn3UkwkMH4Y8P4LQN/Qmt6HerJfPT+D4WmlsciAWcyzNIpno8E+/+8A/lAm4/wMEf6WQ/Tn7F79hGbZto/aU9yHseBx5LZ5ISkYjzoCprktlDIxb4vH2mX5wxzuwhdrSXus2ak2jQN9WDN7fmPx79DQwRWnp4um8DwSWwnrJxQrm3YbniMJr29s2H3+FkottPrf544huza1f8ehW3fo1D9C/s2LYmFcv+jhGa04+s5CqqpX7KyXQ5Sp2GVXmV010WMM8KGGZWVVPL90HbfK4s+PVkCKJKBeGAGHE2Zve7E2nJCNBls5xncS5w1ZWCuw1T1CqzKexzT2yTFHEo0zNLA9+IzCPkVsNLX6obcwliLoAjhPiumDF2+Wed+sMZW85Q1zKU4gMwxRe97Xy1oI6OaszZEtsBJY/d7gTL9/vhl9siqYTXATiOGGAZoZpJked3krrg8kKfXGzNK+WV0AT3Wvvq1eZyO3aS0VsYTQZzByv7f7+9D2b2jvSji9Qe0C62QlUqxwYlGcwRQXVqhFKp7NCai5gS3JLGxrVlq5gvKkbMgYWzEsfdOGEVAsTyNx8IrtKCWIXgMgHSQKQyLeTBHYsia02knCwQgSX8EyExs5KvgZfzRNKrQy28sYtyoXckpy8GhxVhr2IAhDA0+14BgZ4XghM8EzpNgJrEM9y4GqMV4iynNtNuUXNV8vEuGp1Ed1azIbO9gzwH9t6T8ElxPFqAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg","srcSet":"/static/97d66df31989169322ece96b70bfd7a8/9976b/hero-image.jpg 125w,\n/static/97d66df31989169322ece96b70bfd7a8/e19ca/hero-image.jpg 250w,\n/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg 500w,\n/static/97d66df31989169322ece96b70bfd7a8/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/97d66df31989169322ece96b70bfd7a8/842b0/hero-image.webp 125w,\n/static/97d66df31989169322ece96b70bfd7a8/b597c/hero-image.webp 250w,\n/static/97d66df31989169322ece96b70bfd7a8/49c7e/hero-image.webp 500w,\n/static/97d66df31989169322ece96b70bfd7a8/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}},"extension":"png","publicURL":"/static/97d66df31989169322ece96b70bfd7a8/hero-image.png"}},"fields":{"slug":"/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally"}}},"pageContext":{"slug":"/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally"}},"staticQueryHashes":["112401468","1485533831","3750885592","4047814605","679004096"],"slicesMap":{},"matchPath":"/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally"}